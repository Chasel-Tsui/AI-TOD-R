
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <meta property="og:title" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning"/>
  <meta property="og:description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning"/>
  <meta property="og:url" content="https://chasel-tsui.github.io/AI-TOD-R/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <meta name="twitter:description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="remote sensing, object detection, tiny objects">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AI-TOD-R</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><b>Oriented Tiny Object Detection:</b> <br>
               A Dataset, Benchmark, and Dynamic Unbiased Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://chasel-tsui.github.io/Homepage/" target="_blank">Chang Xu</a><sup>*</sup>,</span>
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-vfbyfwAAAAJ&hl=zh-CN" target="_blank">Ruixiang Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-aVL-UQAAAAJ&hl=zh-CN" target="_blank">Wen Yang</a>,</span>
            </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="https://github.com/ZhuHaoranEIS" target="_blank">Haoran Zhu</a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=JkHxI3gAAAAJ&hl=zh-CN" target="_blank">Xu Fang</a>,</span>
                    <span class="author-block">
                      <a href="https://dingjiansw101.github.io" target="_blank">Jian Ding</a>, and </span>
                      <span class="author-block">
                        <a href="https://cs.whu.edu.cn/info/1019/2878.htm" target="_blank">Gui-song Xia</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Wuhan University, </span>
                    <span class="author-block">EPFL, </span>
                    <span class="author-block">KAUST</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Under review</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <p><br></p>
                      <!--<a href="http://www.epfl.ch/labs/eceo/"><img src="static/images/Fig_ECEO.png" width="180px" margin-left="20px" margin-right="20px" alt="ECEO Logo"/></a></span>-->
                      <a href="http://epfl.ch"><img src="static/images/Fig_EPFL.png" width="180px"  margin-left="20px"  margin-right="20px" alt="EPFL Logo"/></a></span>
                      <a href="https://www.whu.edu.cn/"><img src="static/images/Fig_WHU.png" width="90px"  margin-left="20px"  margin-right="20px" alt="WHU Logo"/></a></span>
                      <a href="https://www.kaust.edu.sa/en"><img src="static/images/kaust.png" width="180px"  margin-left="20px" margin-right="20px" alt="NLP Logo"/></a></span>
                  </div>

                  <!---TODO ECEO LOGO, and change logo from red/white invert, put pipeline image-->
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Pre-Print</span>
                      </a>
                    </span>
                    

                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Chasel-Tsui/AI-TOD-R" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://drive.google.com/file/d/1gFaoc5p5whoeDsnsLt6F18O6FHbLGtlK/view?usp=drive_link" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-folder"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

                <div class="container is-max-desktop">
                <div class="columns is-centered">
                <div class="column is-four-fifths">
                <div class="content has-text-justified">
                  <p><br></p>
                <p>
                <div align="center">
                <b style="font-size:21px; ">This work addresses the challenging task of detecting oriented tiny objects by establishing a new dataset and a benchmark, and proposing a dynamic coarse-to-fine learning scheme aimed at scale-unbiased learning.</b> 
                <p><br></p>
                </div>
                <div align="center">
                <img src="static/images/fig1_v3.png" alt="An Overview of This Work" width="70%"/>
                </div> 
                <p><b>Dataset</b>: AI-TOD-R has 28,036 images annotated with 752,460 oriented bounding boxes across 8 classes, it has the smallest mean object size ($10.6^{2}$) among existing oriented object detection datasets. </p>
                <p><b>Benchmark</b>: This benchmark covers both fully-supervised and label efficient (SSOD, SAOD, WSOD) methods. Here, ``L.'', ``U.'', ``S. L.'', and ``C. L.'' denote labelled, unlabelled, sparsely labelled, and coarsely labelled images, respectively.</p> 
                <p><b>Dynamic Unbiased Learning</b>: Compared to prior arts (left), our proposed pipeline (right) mitigates the model's learning bias against oriented tiny objects with a dynamically updated prior and a coarse-to-fine sample learning scheme.</p>
                <!-- ArXiv abstract Link -->
                <!--<span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Motivation</h2>
        <div class="content has-text-justified">
        <p>Images will inevitably contain <b>objects at extremely tiny scales</b> when observations approach the camera's physical limit. Despite extreme, this scenario is ubiquitous in real world from micro-vision (medical, cell imaging) to macro-vision (drone, satellite imagery). In these specialized domains, imaging typically adopts a bird-eye's view to capture object's primary features, making them show arbitrary orientations. These oriented tiny objects pose severe challenges to object detection:
        </p>
        <ul>
          <li><b>Difficult Label Acquisition:</b> The limited appearance information makes the labelling quite difficult.</li>
          <li><b>Weak Feature Representation:</b> The features of oriented tiny objects will be lost in deep networks.</li>
          <li><b>Dense Object Arrangement:</b> There can be thousands of objects per image due to overhead view.</li>
        </ul>
          <div align="center">
            <img src="static/images/applications.png" alt="Application Scenarios" width="100%"/>
          </div>
          <p><br></p>
          <p>
          These challenges give rise to the following problems:
            <ul>
            <li>Existing methods struggle to deliver satisfactory performance. 77% objects in DOTA-v2 are in the size range of $10^{2}$-$50^{2}$ pixels, while the SOTA performance is still lower than 30% $\rm{AP^{@50:5:95}}$.</li>
            <li>Detecting oriented tiny objects is a hard nut to crack in the community, while there is a lack of task-specific datasets and benchmarks to prompt the development of detection methods.</li>
            </ul>
          This motivates us to build a task-specific dataset, benchmark and method for oriented tiny objects.
          </p>
       </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->



<!-- Dataset -->
<section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset</h2>
          <div class="content has-text-justified">
            <p>
            <b>AI-TOD-R</b> is obtained via a semi-automative labelling process, where coarse labels are generated by H2RBox-v2 while fine labels are refined manually. AI-TOD-R contains 28,036 images annotated with 752,460 oriented bounding boxes across 8 classes, it has the smallest mean object size ($10.6^{2}$) among existing oriented object detection datasets.
            </p>
            <figure class="image mod-figure">
              <img src="static/images/aitodr_stat.png" alt="AI-TOD-R Statics" width="100%">
            </figure>
            <p style="font-size:13px; ">
            AI-TOD-R's statistical analysis. In addition to extremely tiny object size, this dataset also shows arbitrary object orientation, dense arrangement, and class imbalance chellenges. 
            </p>

            <p>
            More examples from AI-TOD-R:
            </p>
            <div align="center">
            <img src="static/images/aitodr.png" alt="AI-TOD-R Visualization" width="100%"/>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Dataset -->

  <!-- Paper Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <div class="content has-text-justified">
        <p>The objects' tiny size and low confidence make them easily suppressed or ignored during model training. The vanilla optimization process will inevitably pose them into significantly biased prior setting and biased sample learning dilemmas, severely impeding the detection performance:
          <ul>
            <li><b>Biased prior setting</b>: Most prior positions are deviated from tiny objects' main area under fixed prior setting constraint by feature stride (e.g., 8, 16, 32, 64).</li>
            <li><b>Biased sample learning</b>: Oriented tiny objects are assigned with much fewer number of positive samples compared to otehr objects due to sub-optimal measaurement and assignment strategy.</li>
          </ul>
        </p>
        <p>Towards scale-unbiased sample learning, we reformulate the training process into a Dynamic Coarse-to-Fine Learning (DCFL) pipeline with: 
          <ul>
            <li><b>Dynamic prior setting</b>: Prior positions are dynamically updated to better accommodate tiny objects' extreme shapes.</li>
            <li><b>Coarse-to-fine sample learning</b>: The coarse step warrants sufficient and diverse positive samples for each object. The fine step guarantees the learning quality by fitting each gt with a Dynamic Gaussian Mixture Model as a constraint to select low-quality samples.</li>
          </ul>
        </p>
          <div align="center">
            <img src="static/images/pipeline_pami.png" alt="Method Overview" width="100%"/>
          </div>
          <p><br></p>
       </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title">Benchmark and Experiments</h2>
          <p><br></p>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          <p>
            Experiments are performed on AI-TOD-R for banchmarking, and also on seven other datasets to investigate <b>DCFL</b>'s <b>superiority</b> on detecting oriented tiny objects, <b>adaptability</b> to various architectures, and <b>generalization</b> to different scenarios. 
        </p>
        </div>
        </div>
        </div>


        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>AI-TOD-R Benchmark</b>
          </h4>
        <p>
          We benchmark both fully-supervised paradigms (e.g., architecture, representation, refinement, assignment, backbone) and label-efficient paradigms (e.g., SSOD, SAOD, WSOD) with this dataset.
        </p>
        </div>
        </div>
        </div>
    <title>Table Example</title>
        <style>
            table {
                width: 80%;
                margin: 0 auto;
                border-collapse: collapse;
            }
            th, td {
                padding: 3px;
                text-align: left;
                border: 1px solid black;
            }
            th {
                background-color: #f2f2f2;
            }
            .text-green {
                color: ForestGreen;
            }
        </style>
    </head>
    <body>
    <h2 style="text-align: center;">Main results of fully-supervised methods on AI-TOD-R</h2>
    <table>
        <thead>
            <tr>
                <th>ID</th>
                <th>Method</th>
                <th>Backbone</th>
                <th>Schedule</th>
                <th>AP</th>
                <th>AP<sub>0.5</sub></th>
                <th>AP<sub>0.75</sub></th>
                <th>AP<sub>vt</sub></th>
                <th>AP<sub>t</sub></th>
                <th>AP<sub>s</sub></th>
                <th>AP<sub>m</sub></th>
                <th>#Params.</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td colspan="12"><em>Architecture:</em></td>
            </tr>
            <tr>
                <td>1</td>
                <td>RetinaNet-O</td>
                <td>R50</td>
                <td>1×</td>
                <td>7.3</td>
                <td>23.9</td>
                <td>1.8</td>
                <td>2.2</td>
                <td>5.9</td>
                <td>11.1</td>
                <td>15.4</td>
                <td>36.3M</td>
            </tr>
            <tr>
                <td>2</td>
                <td>FCOS-O</td>
                <td>R50</td>
                <td>1×</td>
                <td>11.0</td>
                <td>33.6</td>
                <td>3.7</td>
                <td>3.0</td>
                <td>8.9</td>
                <td>15.7</td>
                <td>22.0</td>
                <td>31.9M</td>
            </tr>
            <tr>
                <td>3</td>
                <td>Faster R-CNN-O</td>
                <td>R50</td>
                <td>1×</td>
                <td>10.2</td>
                <td>30.8</td>
                <td>3.6</td>
                <td>0.6</td>
                <td>7.8</td>
                <td>19.0</td>
                <td>22.9</td>
                <td>41.1M</td>
            </tr>
            <tr>
                <td>4</td>
                <td>RoI Transformer</td>
                <td>R50</td>
                <td>1×</td>
                <td>10.5</td>
                <td>34.0</td>
                <td>2.2</td>
                <td>1.1</td>
                <td>8.8</td>
                <td>16.9</td>
                <td>20.3</td>
                <td>55.1M</td>
            </tr>
            <tr>
                <td>5</td>
                <td>Oriented R-CNN</td>
                <td>R50</td>
                <td>1×</td>
                <td>11.2</td>
                <td>33.2</td>
                <td>4.3</td>
                <td>0.6</td>
                <td>9.1</td>
                <td>19.5</td>
                <td>23.2</td>
                <td>41.1M</td>
            </tr>
            <tr>
                <td>6</td>
                <td>Deformable DETR-O</td>
                <td>R50</td>
                <td>1×</td>
                <td>8.4</td>
                <td>26.7</td>
                <td>2.0</td>
                <td>4.8</td>
                <td>9.3</td>
                <td>8.6</td>
                <td>7.3</td>
                <td>40.8M</td>
            </tr>
            <tr>
                <td>7</td>
                <td>ARS-DETR</td>
                <td>R50</td>
                <td>1×</td>
                <td>14.3</td>
                <td>41.1</td>
                <td>5.8</td>
                <td>6.3</td>
                <td>14.5</td>
                <td>17.6</td>
                <td>18.7</td>
                <td>41.1M</td>
            </tr>
            <tr>
                <td colspan="12"><em>Representation:</em></td>
            </tr>
            <tr>
                <td>8</td>
                <td>KLD (RetinaNet-O)</td>
                <td>R50</td>
                <td>1×</td>
                <td>7.8</td>
                <td>24.8</td>
                <td>2.3</td>
                <td>3.1</td>
                <td>6.7</td>
                <td>10.3</td>
                <td>15.8</td>
                <td>36.3M</td>
            </tr>
            <tr>
                <td>9</td>
                <td>KFIoU (RetinaNet-O)</td>
                <td>R50</td>
                <td>1×</td>
                <td>8.1</td>
                <td>25.2</td>
                <td>2.8</td>
                <td>2.0</td>
                <td>6.6</td>
                <td>12.3</td>
                <td>17.1</td>
                <td>36.3M</td>
            </tr>
            <tr>
                <td>10</td>
                <td>Oriented RepPoints</td>
                <td>R50</td>
                <td>1×</td>
                <td>13.0</td>
                <td>40.3</td>
                <td>4.2</td>
                <td>5.2</td>
                <td>12.2</td>
                <td>16.8</td>
                <td>21.4</td>
                <td>36.6M</td>
            </tr>
            <tr>
                <td>11</td>
                <td>PSC (RetinaNet-O)</td>
                <td>R50</td>
                <td>1×</td>
                <td>4.5</td>
                <td>15.8</td>
                <td>1.2</td>
                <td>1.0</td>
                <td>3.7</td>
                <td>8.2</td>
                <td>12.7</td>
                <td>36.4M</td>
            </tr>
            <tr>
                <td>12</td>
                <td>Gliding Vertex</td>
                <td>R50</td>
                <td>1×</td>
                <td>8.1</td>
                <td>27.4</td>
                <td>2.1</td>
                <td>0.9</td>
                <td>6.7</td>
                <td>14.7</td>
                <td>17.9</td>
                <td>41.1M</td>
            </tr>
            <tr>
                <td colspan="12"><em>Refinement:</em></td>
            </tr>
            <tr>
                <td>13</td>
                <td>R<sup>3</sup>Det</td>
                <td>R50</td>
                <td>1×</td>
                <td>8.1</td>
                <td>24.4</td>
                <td>1.8</td>
                <td>0.4</td>
                <td>6.6</td>
                <td>15.2</td>
                <td>19.1</td>
                <td>38.6M</td>
            </tr>
            <tr>
              <td>14</td>
              <td>S<sup>2</sup>A-Net</td>
              <td>R50</td>
              <td>1×</td>
              <td>10.8</td>
              <td>33.4</td>
              <td>3.3</td>
              <td>4.3</td>
              <td>11.2</td>
              <td>13.0</td>
              <td>16.0</td>
              <td>38.6M</td>
          </tr>
            <tr>
                <td colspan="12"><em>Assignment:</em></td>
            </tr>
            <tr>
                <td>15</td>
                <td>ATSS-O</td>
                <td>R50</td>
                <td>1×</td>
                <td>10.9</td>
                <td>33.8</td>
                <td>3.1</td>
                <td>2.7</td>
                <td>8.9</td>
                <td>15.5</td>
                <td>19.4</td>
                <td>36.0M</td>
            </tr>
            <tr>
              <td>16</td>
              <td>SASM </td>
              <td>R50</td>
              <td>1×</td>
              <td>11.4</td>
              <td>35.0</td>
              <td>3.7</td>
              <td>3.6</td>
              <td>10.2</td>
              <td>15.4</td>
              <td>19.8</td>
              <td>36.6M</td>
          </tr>
          <tr>
              <td>17</td>
              <td>CFA</td>
              <td>R50</td>
              <td>1×</td>
              <td>12.4</td>
              <td>38.7</td>
              <td>4.0</td>
              <td>5.0</td>
              <td>11.9</td>
              <td>16.5</td>
              <td>18.8</td>
              <td>36.6M</td>
          </tr>
          <tr>
            <td colspan="12"><em>Backbone:</em></td>
          </tr>
          <tr>
              <td>18</td>
              <td>Oriented R-CNN</td>
              <td>R101</td>
              <td>1×</td>
              <td>11.2</td>
              <td>33.0</td>
              <td>4.1</td>
              <td>0.5</td>
              <td>8.9</td>
              <td>19.8</td>
              <td>24.4</td>
              <td>60.1M</td>
          </tr>
          <tr>
              <td>19</td>
              <td>Oriented R-CNN</td>
              <td>Swin-T</td>
              <td>1×</td>
              <td>12.0</td>
              <td>34.6</td>
              <td>4.6</td>
              <td>0.7</td>
              <td>9.9</td>
              <td>20.8</td>
              <td>25.3</td>
              <td>44.8M</td>
          </tr>
          <tr>
              <td>20</td>
              <td>Oriented R-CNN</td>
              <td>LSKNet-T</td>
              <td>1×</td>
              <td>11.1</td>
              <td>33.4</td>
              <td>3.8</td>
              <td>0.6</td>
              <td>9.2</td>
              <td>18.9</td>
              <td>22.6</td>
              <td>21.0M</td>
          </tr>
          <tr>
              <td>21</td>
              <td>ReDet</td>
              <td>ReR50</td>
              <td>1×</td>
              <td>11.6</td>
              <td>32.8</td>
              <td>4.8</td>
              <td>1.4</td>
              <td>9.5</td>
              <td>19.4</td>
              <td>23.2</td>
              <td>31.6M</td>
          </tr>
          <tr>
            <td colspan="12"><em>Ours:</em></td>
          </tr>
          <tr>
              <td>22</td>
              <td>DCFL (RetinaNet-O)</td>
              <td>R50</td>
              <td>1×</td>
              <td>12.3 <span class="green">(+5.0)</span></td>
              <td>36.7 <span class="green">(+12.8)</span></td>
              <td>4.5 <span class="green">(+2.7)</span></td>
              <td>4.3</td>
              <td>10.7</td>
              <td>17.2</td>
              <td>22.2</td>
              <td>36.1M</td>
          </tr>
          <tr>
              <td>23</td>
              <td>DCFL (RetinaNet-O)</td>
              <td>R50</td>
              <td>40e</td>
              <td>15.2 <span class="green">(+7.9)</span></td>
              <td>44.9 <span class="green">(+21.0)</span></td>
              <td>5.1 <span class="green">(+3.3)</span></td>
              <td>4.9</td>
              <td>13.1</td>
              <td>19.7</td>
              <td>25.9</td>
              <td>36.1M</td>
          </tr>
          <tr>
              <td>24</td>
              <td>DCFL (Oriented R-CNN)</td>
              <td>R50</td>
              <td>1×</td>
              <td>15.7 <span class="green">(+4.5)</span></td>
              <td>47.0 <span class="green">(+13.8)</span></td>
              <td>5.8 <span class="green">(+1.5)</span></td>
              <td>6.3</td>
              <td>14.8</td>
              <td>19.6</td>
              <td>22.5</td>
              <td>41.1M</td>
          </tr>
          <tr>
              <td>25</td>
              <td>DCFL (Oriented R-CNN)</td>
              <td>R50</td>
              <td>40e</td>
              <td>17.1 <span class="green">(+5.9)</span></td>
              <td>49.0 <span class="green">(+15.8)</span></td>
              <td>7.2 <span class="green">(+2.9)</span></td>
              <td>6.4</td>
              <td><b>16.0</b></td>
              <td>21.6</td>
              <td>24.9</td>
              <td>41.1M</td>
          </tr>
          <tr>
            <td>26</td>
            <td>DCFL ($\rm{S^2A}$-Net)</td>
            <td>R50</td>
            <td>1×</td>
            <td>13.7 <span class="green">(+2.9)</span></td>
            <td>39.7 <span class="green">(+6.3)</span></td>
            <td>5.3 <span class="green">(+2.0)</span></td>
            <td>4.7</td>
            <td>12.4</td>
            <td>18.6</td>
            <td>22.6</td>
            <td>38.6M</td>
        </tr>
        <tr>
            <td>27</td>
            <td>DCFL ($\rm{S^2A}$-Net)</td>
            <td>R50</td>
            <td>40e</td>
            <td><b>17.5</b> <span class="green">(+6.7)</span></td>
            <td><b>49.6</b> <span class="green">(+16.2)</span></td>
            <td><b>7.9</b> <span class="green">(+4.6)</span></td>
            <td><b>6.5</b></td>
            <td>15.7</td>
            <td><b>22.6</b></td>
            <td><b>27.4</b></td>
            <td>38.6M</td>
        </tr>
        </tbody>
    </table>
    </body>
    <br>
    <table style="width: 80%; border-collapse: collapse;">
      <caption>Main results of label-efficient methods on AI-TOD-R. </caption>
      <thead>
          <tr>
              <th rowspan="2">Method</th>
              <th rowspan="2">Category</th>
              <th colspan="4">10% OBB</th>
              <th colspan="4">20% OBB</th>
              <th colspan="4">30% OBB</th>
              <th colspan="4">100% HBB</th>
          </tr>
          <tr>
              <th>AP</th>
              <th>AP<sub>0.5</sub></th>
              <th>AP<sub>vt</sub></th>
              <th>AP<sub>t</sub></th>
              <th>AP</th>
              <th>AP<sub>0.5</sub></th>
              <th>AP<sub>vt</sub></th>
              <th>AP<sub>t</sub></th>
              <th>AP</th>
              <th>AP<sub>0.5</sub></th>
              <th>AP<sub>vt</sub></th>
              <th>AP<sub>t</sub></th>
              <th>AP</th>
              <th>AP<sub>0.5</sub></th>
              <th>AP<sub>vt</sub></th>
              <th>AP<sub>t</sub></th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td>Unbiased Teacher</td>
              <td>SSOD</td>
              <td>7.6</td>
              <td>24.7</td>
              <td>0.4</td>
              <td>6.0</td>
              <td>8.1</td>
              <td>24.7</td>
              <td>0.5</td>
              <td>6.1</td>
              <td>8.1</td>
              <td>25.4</td>
              <td>0.4</td>
              <td>6.1</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
          </tr>
          <tr>
              <td>Soft Teacher</td>
              <td>SSOD</td>
              <td>9.4</td>
              <td>29.0</td>
              <td>0.3</td>
              <td>7.6</td>
              <td>10.2</td>
              <td>31.1</td>
              <td>0.5</td>
              <td>7.9</td>
              <td>10.4</td>
              <td>32.2</td>
              <td>0.6</td>
              <td>7.8</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
          </tr>
          <tr>
              <td>SOOD</td>
              <td>SSOD</td>
              <td>9.4</td>
              <td>29.3</td>
              <td>2.8</td>
              <td>8.1</td>
              <td>12.1</td>
              <td>35.7</td>
              <td>3.5</td>
              <td>10.2</td>
              <td>13.0</td>
              <td>38.8</td>
              <td>3.9</td>
              <td>11.1</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
          </tr>
          <tr>
              <td>Co-mining</td>
              <td>SAOD</td>
              <td>6.4</td>
              <td>20.4</td>
              <td>0.5</td>
              <td>4.4</td>
              <td>8.0</td>
              <td>24.1</td>
              <td>0.4</td>
              <td>6.1</td>
              <td>8.2</td>
              <td>25.0</td>
              <td>0.4</td>
              <td>6.8</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
          </tr>
          <tr>
              <td>H2R-Box</td>
              <td>WSOD</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>11.4</td>
              <td>39.1</td>
              <td>3.4</td>
              <td>9.4</td>
          </tr>
          <tr>
              <td>H2R-Box-v2</td>
              <td>WSOD</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>11.7</td>
              <td>38.2</td>
              <td>4.6</td>
              <td>9.5</td>
          </tr>
      </tbody>
  </table>
        <br>
        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>DCFL's Detection Performance</b>
          </h4>
        <p>
            DCFL shows superior performance on diverse object detection scenarios, including <b>small oriented object detection</b> (<a href="https://chasel-tsui.github.io/AI-TOD-R/">AI-TOD-R</a>, <a href="https://shaunyuan22.github.io/SODA/">SODA-A</a>), <b>oriented object detection with massive tiny objects</b> (<a href="https://captain-whu.github.io/DOTA/dataset.html">DOTA-v1.5</a>, <a href="https://captain-whu.github.io/DOTA/dataset.html">DOTA-v2</a>), <b>generic oriented object detection</b> (<a href="https://captain-whu.github.io/DOTA/dataset.html">DOTA-v1</a>, <a href="https://github.com/jbwang1997/AOPG">DIOR-R</a>), and <b>horizontal object detection</b> (<a href="https://github.com/VisDrone/VisDrone-Dataset">VisDrone</a>, <a href="https://cocodataset.org/">MS COCO</a>). Notably, DCFL will not introduce additional inference cost compared to the baseline. Please refer to the paper for details.
        </p>
        </div>
        </div>
        </div>

        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>Visualization of Detection Results</b>
          </h4>
        <p>
          DCFL significantly improves the detection performance on oriented tiny objects by suppressing both false negative and false positive predictions.
        </p>
        </div>
        </div>
        </div>

        <p><br></p>
        <div align="center">
          <img src="static/images/vis_pred.png" alt="MY ALT TEXT" width="80%"/>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p style="font-size:13px; ">
          Visualization analysis of the predicted results. The first row: Oriented R-CNN, the second row: DCFL. True positive, false negative, and false positive predictions are shown in green, red, and blue boxes, respectively.
        </p>
        </div>
        </div>
        </div>


        <p><br></p>

    </div>
  </div>
  </div>
  </section>
  <!-- End image carousel -->

<!-- Paper poster 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xu2024oriented,
  title={Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning},
  author={Xu, Chang and Zhang, Ruixiang and Yang, Wen and Zhu, Haoran and Xu, Fang and Ding, Jian and Xia, Gui-Song},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the template of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

