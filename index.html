
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <meta property="og:title" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning"/>
  <meta property="og:description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning"/>
  <meta property="og:url" content="https://chasel-tsui.github.io/AI-TOD-R/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <meta name="twitter:description" content="Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="remote sensing, object detection, tiny objects">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AI-TOD-R</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><b>Oriented Tiny Object Detection:</b> <br>
               A Dataset, Benchmark, and Dynamic Unbiased Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://chasel-tsui.github.io/Homepage/" target="_blank">Chang Xu</a><sup>*</sup>,</span>
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-vfbyfwAAAAJ&hl=zh-CN" target="_blank">Ruixiang Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-aVL-UQAAAAJ&hl=zh-CN" target="_blank">Wen Yang</a>,</span>
            </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="https://github.com/ZhuHaoranEIS" target="_blank">Haoran Zhu</a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=JkHxI3gAAAAJ&hl=zh-CN" target="_blank">Xu Fang</a>,</span>
                    <span class="author-block">
                      <a href="https://dingjiansw101.github.io" target="_blank">Jian Ding</a>, and </span>
                      <span class="author-block">
                        <a href="https://cs.whu.edu.cn/info/1019/2878.htm" target="_blank">Gui-song Xia</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Wuhan University, </span>
                    <span class="author-block">EPFL, </span>
                    <span class="author-block">KAUST</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Under review</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <p><br></p>
                      <!--<a href="http://www.epfl.ch/labs/eceo/"><img src="static/images/Fig_ECEO.png" width="180px" margin-left="20px" margin-right="20px" alt="ECEO Logo"/></a></span>-->
                      <a href="http://epfl.ch"><img src="static/images/Fig_EPFL.png" width="180px"  margin-left="20px"  margin-right="20px" alt="EPFL Logo"/></a></span>
                      <a href="https://www.whu.edu.cn/"><img src="static/images/Fig_WHU.png" width="90px"  margin-left="20px"  margin-right="20px" alt="WHU Logo"/></a></span>
                      <a href="https://www.kaust.edu.sa/en"><img src="static/images/kaust.png" width="180px"  margin-left="20px" margin-right="20px" alt="NLP Logo"/></a></span>
                  </div>

                  <!---TODO ECEO LOGO, and change logo from red/white invert, put pipeline image-->
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Pre-Print</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!--<span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://chasel-tsui.github.io/AI-TOD-R/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <div class="container is-max-desktop">
                <div class="columns is-centered">
                <div class="column is-four-fifths">
                <div class="content has-text-justified">
                  <p><br></p>
                <p>
                <div align="center">
                <b style="font-size:21px; ">This work addresses the challenging task of detecting oriented tiny objects by establishing a new dataset and a benchmark, and proposing a dynamic coarse-to-fine learning scheme aimed at scale-unbiased learning.</b> 
                <p><br></p>
                </div>
                <div align="center">
                <img src="static/images/fig1_v3.png" alt="An Overview of This Work" width="70%"/>
                </div> 
                <p><b>Dataset</b>: AI-TOD-R has 28,036 images annotated with 752,460 oriented bounding boxes across 8 classes, it has the smallest mean object size ($10.6^{2}$) among existing oriented object detection datasets. </p>
                <p><b>Benchmark</b>: This benchmark covers both fully-supervised and label efficient (SSOD, SAOD, WSOD) methods. Here, ``L.'', ``U.'', ``S. L.'', and ``C. L.'' denote labelled, unlabelled, sparsely labelled, and coarsely labelled images, respectively.</p> 
                <p><b>Dynamic Unbiased Learning</b>: Compared to prior arts (left), our proposed pipeline (right) mitigates the model's learning bias against oriented tiny objects with a dynamically updated prior and a coarse-to-fine sample learning scheme.</p>
                <!-- ArXiv abstract Link -->
                <!--<span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Motivation</h2>
        <div class="content has-text-justified">
        <p>Images will inevitably contain <b>objects at extremely tiny scales</b> when observations approach the camera's physical limit. Despite extreme, this scenario is ubiquitous in real world from micro-vision (medical, cell imaging) to macro-vision (drone, satellite imagery). In these specialized domains, imaging typically adopts a bird-eye's view to capture object's primary features, making them show arbitrary orientations. These oriented tiny objects pose severe challenges to object detection:
        </p>
        <ul>
          <li><b>Difficult Label Acquisition:</b> The limited appearance information makes the labelling quite difficult.</li>
          <li><b>Weak Feature Representation:</b> The features of oriented tiny objects will be lost in deep networks.</li>
          <li><b>Dense Object Arrangement:</b> There can be thousands of objects per image due to overhead view.</li>
        </ul>
          <div align="center">
            <img src="static/images/applications.png" alt="Application Scenarios" width="100%"/>
          </div>
          <p><br></p>
          <p>
          These challenges give rise to the following problems:
            <ul>
            <li>Existing methods struggle to deliver satisfactory performance. 77% objects in DOTA-v2 are in the size range of $10^{2}$-$50^{2}$ pixels, while the SOTA performance is still lower than 30% $\rm{AP^{@50:5:95}}$.</li>
            <li>Detecting oriented tiny objects is a hard nut to crack in the community, while there is a lack of task-specific datasets and benchmarks to prompt the development of detection methods.</li>
            </ul>
          This motivates us to build a task-specific dataset, benchmark and method for oriented tiny objects.
          </p>
       </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->



<!-- Dataset -->
<section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset</h2>
          <div class="content has-text-justified">
            <p>
            <b>AI-TOD-R</b> is obtained via a semi-automative labelling process, where coarse labels are genertaed H2RBox-v2 while fine labels are refined manually. AI-TOD-R contains 28,036 images annotated with 752,460 oriented bounding boxes across 8 classes, it has the smallest mean object size ($10.6^{2}$) among existing oriented object detection datasets.
            </p>
            <figure class="image mod-figure">
              <img src="static/images/aitodr_stat.png" alt="AI-TOD-R Statics" width="100%">
            </figure>
            <p style="font-size:13px; ">
            AI-TOD-R's statistical analysis. In addition to extremely tiny object size, this dataset also shows arbitrary object orientation, dense arrangement, and class imbalance chellenges. 
            </p>

            <p>
            More examples from AI-TOD-R:
            </p>
            <div align="center">
            <img src="static/images/aitodr.png" alt="AI-TOD-R Visualization" width="100%"/>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Dataset -->

  <!-- Paper Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <div class="content has-text-justified">
        <p>Towards unbiased sample learning, we reformulate this process into a dynamic coarse-to-fine learning pipeline based on the dynamic priors. The coarse step works in an object-centric way, where we construct a coarse positive candidate bag to warrant sufficient and diverse positive samples for each object. The fine step aims at guaranteeing the learning quality, where we fit each gt with a Dynamic Gaussian Mixture Model (DGMM) as a constraint to select low-quality samples.
        </p>
          <div align="center">
            <img src="static/images/pipeline_pami.png" alt="Method Overview" width="100%"/>
          </div>
          <p><br></p>
       </div>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <h2 class="title">Benchmark and Experiments</h2>
          <p><br></p>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          <p>
            Experiments are performed on eight cross-view geolocalization benchmarks to investigate <b>ConGeo</b>'s <b>robustness</b>, <b>adaptability</b>, and <b>generalization ability</b>. Analysis on orientation invariance and activation map further comfirms <b>ConGeo</b>'s robustness.
        </p>
        </div>
        </div>
        </div>


        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>AI-TOD-R Dataset</b>
          </h4>
        <p>
          Using a single model, ConGeo outperforms SOTA FoV-specific or orientation-specific models under ground-view variations and achieves leading performance under North alignment. 
        </p>
        </div>
        </div>
        </div>

        <p><br></p>

        <div align="center">
          <img src="static/images/Fig_RFoVs.png" alt="MY ALT TEXT" width="30%"/>
        </div>
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <div align="center">
        <p style="font-size:13px; ">
            Comparison with SOTA FoV-specific methods on different settings on the CVUSA dataset.
        </p>
        </div>
        </div>
        </div>
        </div>

        <p><br></p>
        <div align="center">
          <img src="static/images/retrieval_results.gif" alt="MY ALT TEXT" width="70%"/>
        </div>
        <p><br></p>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <div align="center">
        <p style="font-size:13px; ">
            Retrieval results of the baseline model and of ConGeo undering the North-aligned setting and limited FoV setting.
        </p>
        </div>
        </div>
        </div>
        </div>

        <p><br></p>





        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>Small Oriented Object Detection</b>
          </h4>
        <p>
            ConGeo shows effectiveness compared with different data augmentation methods.
        </p>
        </div>
        </div>
        </div>

        <p><br></p>
        <div align="center">
          <img src="static/images/aug_hist.png" alt="MY ALT TEXT" width="80%"/>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p style="font-size:13px; ">
            Comparison on unknown orientation setting and limited FoV setting between ConGeo and different data augmentation methods on the CVUSA dataset. “Shift” denotes using shifted query images and “FoV” denotes using query images of limited FoVs, “Rotate” randomly rotating aerial images as data augmentation.
        </p>
        </div>
        </div>
        </div>

        <p><br></p>

      

        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>Oriented Object Detection</b>
          </h4>
        <p>
            <b>ConGeo</b> generalizes better on unseen ground view variations than baseline model and baseline model with data augmentation.
        </p>
        </div>
        </div>
        </div>

        <p><br></p>
        <div align="center">
          <img src="static/images/unseen_hist.png" alt="MY ALT TEXT" width="80%"/>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p style="font-size:13px; ">
            Comparison on unseen ground view variations (e.g., Random FoVs, Random Zooming, Gaussian Noise, and Motion Blur) between ConGeo and baselines on the CVUSA dataset. “DA” means data augmentation.
        </p>
        </div>
        </div>
        </div>


        <p><br></p>


        <div class="item">
        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <h4 class="subtitle">
            <b>Analysis: How does ConGeo achieve robustness?</b>
          </h4>
        <p>
            Orientation invariance analysis that showcases models’ vulnerabilities to orientation shifts and activation map visualization that investigates the models’ focus. 
        </p>
        </div>
        </div>
        </div>

        <p><br></p>
        <div align="center">
          <img src="static/images/Fig_ROrientation.png" alt="MY ALT TEXT" width="45%"/>
        </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p style="font-size:13px; ">
            ConGeo shows better orientation invariance. We cyclically shift the ground view with an angle (x-axis) as the model’s input to test its retrieval performance. Note that “N-A” denotes the North-aligned setting and “DA” means data augmentation.
        </p>
        </div>
        </div>
        </div>

          <p><br></p>
          <!-- Your image here -->
          <div align="center">
          <img src="static/images/attention.gif" alt="MY ALT TEXT" width="70%"/>
          </div>

        <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <div align="center">
        <p style="font-size:13px; ">
            ConGeo’s activation areas are more consistent across ground view shifts.
        </p>
        </div>
        </div>
        </div>
        </div>

        <p><br></p>
        </div> 

        <p><br></p>

    </div>
  </div>
  </div>
  </section>
  <!-- End image carousel -->

<!-- Paper poster 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xu2024oriented,
  title={Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic Unbiased Learning},
  author={Xu, Chang and Zhang, Ruixiang and Yang, Wen and Zhu, Haoran and Xu, Fang and Xia, Gui-Song},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the template of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

